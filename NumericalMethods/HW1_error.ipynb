{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "header1",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%precision 16\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "header2",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, restart the kernel (in the menubar, select Kernel $\\rightarrow$ Restart) and then run all cells (in the menubar, select Cell $\\rightarrow$ Run All).\n",
    "\n",
    "Make sure you fill in any place that says YOUR CODE HERE or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name: Anshul Doshi (ad3222)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "header3",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# HW 1 - Forms of Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Q1",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "source": [
    "## Question 1\n",
    "\n",
    "Find the absolute error, relative error, and decimal precision (number of significant decimal digits) for the following $f$ and approximations $\\hat{f}$.  Note that here we may also mean precision as compared to $f$.  In these cases use the absolute error to help define $\\hat{f}$'s precision (each worth 5 points).\n",
    "\n",
    "**(a)** $f = \\pi$ and $\\hat{f} = 3.14$\n",
    "\n",
    "**(b)** $f = \\pi$ and $\\hat{f} = 22 / 7$\n",
    "\n",
    "**(c)** $f = \\log (n!)$ and $\\hat{f} = n~log(n) - n$ for $n = 5, ~~ 10, ~~ 100$ (Stirling's approximation)\n",
    "\n",
    "**(d)** $f = e^x$ and $\\hat{f} = T_n(x)$ where $T_n(x)$ is the Taylor polynomial approximation to $e^x$ expanded about $x = 0$.  Consider $N = 1, 2, 3$.  What vaule of $N$ is required for this approximation to be good to 6 digits of decimal precision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part A Solution\n",
      "\n",
      "decimal precision p is: 3.0\n",
      "[3.0, 0.001593, 0.00050699999999999996]\n",
      "\n",
      "\n",
      "Part B Solution\n",
      "\n",
      "decimal precision p is: 3.0\n",
      "[3.0, 0.0012639999999999999, 0.00040199999999999996]\n",
      "\n",
      "\n",
      "Part C Solution\n",
      "\n",
      "decimal precision p is: 0.0\n",
      "[0.0, 1.74, 0.36399999999999999]\n",
      "decimal precision p is: 1.0\n",
      "[1.0, 2.0790000000000002, 0.13800000000000001]\n",
      "decimal precision p is: 2.0\n",
      "[2.0, 3.222, 0.0090000000000000011]\n",
      "\n",
      "\n",
      "Part D Solution\n",
      "\n",
      "approx is  1.0\n",
      "decimal precision p is: 0.0\n",
      "[0.0, 1.718, 0.63200000000000001]\n",
      "decimal precision p is: 0.0\n",
      "approx is  2.0\n",
      "decimal precision p is: 1.0\n",
      "[1.0, 0.71830000000000005, 0.26419999999999999]\n",
      "decimal precision p is: 1.0\n",
      "approx is  2.5\n",
      "decimal precision p is: 1.0\n",
      "[1.0, 0.21830000000000002, 0.08030000000000001]\n",
      "decimal precision p is: 1.0\n",
      "approx is  2.66666666667\n",
      "decimal precision p is: 2.0\n",
      "[2.0, 0.051620000000000006, 0.01899]\n",
      "decimal precision p is: 2.0\n",
      "approx is  2.70833333333\n",
      "decimal precision p is: 2.0\n",
      "[2.0, 0.0099480000000000002, 0.0036599999999999996]\n",
      "decimal precision p is: 2.0\n",
      "approx is  2.71666666667\n",
      "decimal precision p is: 3.0\n",
      "[3.0, 0.0016149999999999999, 0.00059400000000000002]\n",
      "decimal precision p is: 3.0\n",
      "approx is  2.71805555556\n",
      "decimal precision p is: 4.0\n",
      "[4.0, 0.0002263, 8.3199999999999989e-05]\n",
      "decimal precision p is: 4.0\n",
      "approx is  2.71825396825\n",
      "decimal precision p is: 5.0\n",
      "[5.0, 2.7860000000000001e-05, 1.025e-05]\n",
      "decimal precision p is: 5.0\n",
      "approx is  2.71827876984\n",
      "decimal precision p is: 6.0\n",
      "[6.0, 3.0590000000000003e-06, 1.125e-06]\n",
      "decimal precision p is: 6.0\n",
      " Solution --> N = 8\n"
     ]
    }
   ],
   "source": [
    "import sympy\n",
    "\n",
    "def get_error(true_val, approx_val):\n",
    "    f_hat = approx_val\n",
    "    f = true_val\n",
    "\n",
    "    abs_error = abs(f - f_hat)\n",
    "    rel_error = abs_error / f\n",
    "\n",
    "    #decimal percision\n",
    "    p = numpy.round(-numpy.log10(numpy.abs(rel_error)))    \n",
    "    print \"decimal precision p is:\" , p\n",
    "\n",
    "    n = numpy.floor(numpy.log10(abs_error))+1.0-4\n",
    "    abs_error = round(10.0**-n*abs_error)*10.0**n\n",
    "    rel_error = round(10.0**-n*rel_error)*10.0**n\n",
    "    return [p, abs_error,rel_error]\n",
    "\n",
    "#part a\n",
    "print \"Part A Solution\\n\"\n",
    "print get_error(numpy.pi,3.14)\n",
    "print '\\n' \n",
    "\n",
    "#part b\n",
    "print \"Part B Solution\\n\"\n",
    "print get_error(numpy.pi,22.0/7.0)\n",
    "print '\\n' \n",
    "\n",
    "#part c\n",
    "print \"Part C Solution\\n\"\n",
    "N = [5,10,100]\n",
    "for n in N:\n",
    "    print get_error(numpy.log(float(numpy.math.factorial(n))),(n*numpy.log(n)) - n)\n",
    "print '\\n' \n",
    "\n",
    "#part d\n",
    "print \"Part D Solution\\n\"\n",
    "T_n = 0\n",
    "for n in xrange(0,15):\n",
    "    #p = sympy.mpmath.taylor(sympy.exp, 0.0, n)\n",
    "    #approx = sympy.mpmath.polyval(p[::-1], 1.0)\n",
    "    T_n += (1.0 / numpy.math.factorial(n))\n",
    "    print 'approx is ' , T_n\n",
    "    print get_error(numpy.exp(1.0), T_n)\n",
    "    if get_error(numpy.exp(1.0), T_n)[0] == 6.0:\n",
    "        print \" Solution --> N =\", n \n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "A1",
     "locked": false,
     "points": 20,
     "solution": true
    }
   },
   "source": [
    "Absolute Error:  $e = |f - \\hat{f}|$\n",
    "\n",
    "Relative Error:  $r = \\frac{e}{|f|} = \\frac{|f - \\hat{f}|}{|f|}$\n",
    "\n",
    "See above for precise answers.\n",
    "\n",
    "**(a)** \n",
    "\n",
    "    Decimal Precision = 3\n",
    "\n",
    "    Absolute error:\n",
    "\n",
    "$$|\\pi - 3.14 | = 1.59 \\cdot 10^{-3}$$\n",
    "\n",
    "    Relative Error:\n",
    "\n",
    "$$\\frac{|\\pi - 3.14 |}{|3.14|} = 5.06 \\cdot 10^{-4}$$\n",
    "\n",
    "**(b)** \n",
    "\n",
    "    Decimal Precision = 3\n",
    "\n",
    "    Absolute error:\n",
    "\n",
    "$$|\\pi - \\frac{22}{7} | = 1.26 \\cdot 10^{-3}$$\n",
    "\n",
    "    Relative Error:\n",
    "\n",
    "$$\\frac{|\\pi - \\frac{22}{7} |}{|\\frac{22}{7}|} = 4.02 \\cdot 10^{-4}$$\n",
    "\n",
    "**(c)** \n",
    "\n",
    "**N = 5**\n",
    "\n",
    "    Decimal Precision = 0\n",
    "\n",
    "    Absolute error:\n",
    "\n",
    "$$|log(n!) - (nlog(n) - n) | = 1.74 \\approx 1$$\n",
    "\n",
    "    Relative Error:\n",
    "\n",
    "$$\\frac{|log(n!) - (nlog(n) - n) |}{(nlog(n) - n) } = .363 \\approx 0$$\n",
    "\n",
    "**N = 10**\n",
    "\n",
    "    Decimal Precision = 1\n",
    "\n",
    "    Absolute error:\n",
    "\n",
    "$$|log(n!) - (nlog(n) - n) | = 2.079 \\approx 2$$\n",
    "\n",
    "    Relative Error:\n",
    "\n",
    "$$\\frac{|log(n!) - (nlog(n) - n) |}{(nlog(n) - n) } = .13 \\approx 0 $$\n",
    "\n",
    "**N = 100**\n",
    "\n",
    "    Decimal Precision = 2\n",
    "\n",
    "    Absolute error:\n",
    "\n",
    "$$|log(n!) - (nlog(n) - n) | = 3.22$$\n",
    "\n",
    "    Relative Error:\n",
    "\n",
    "$$\\frac{|log(n!) - (nlog(n) - n) |}{(nlog(n) - n) } = .0090 $$\n",
    "\n",
    "**(d)**\n",
    "\n",
    "As per the above function, N = 8 for the decimal percision to be 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Q2",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "Q2-a",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**(a)** (10) Write a Python program to compute\n",
    "\n",
    "$$S_N = \\sum^N_{n=1} \\left [ \\frac{1}{n} - \\frac{1}{n+1} \\right ] = \\sum^N_{n=1} \\frac{1}{n (n + 1)}$$\n",
    "\n",
    "once using the first summation and once using the second for $N = 10, 10^2, \\ldots , 10^7$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "A2-a",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sum_1(N):\n",
    "    \"\"\"Compute the summation S_N defined as\n",
    "    \n",
    "    \\sum^N_{n=1} \\left [ \\frac{1}{n} - \\frac{1}{n+1} \\right ]\n",
    "    \n",
    "    :Input:\n",
    "     *N* (int) The upper bound on the summation.\n",
    "    \n",
    "    Returns Sn (float)\n",
    "    \"\"\"\n",
    "    \n",
    "    # INSERT CODE HERE\n",
    "    Sn = 0\n",
    "    for n in xrange(1,N+1):\n",
    "        Sn += (1.0/n) - (1.0/(n+1))\n",
    "    return Sn\n",
    "    \n",
    "def sum_2(N):\n",
    "    \"\"\"Compute the summation S_N defined as\n",
    "    \n",
    "    \\sum^N_{n=1} \\frac{1}{n (n + 1)}\n",
    "    \n",
    "    :Input:\n",
    "     *N* (int) The upper bound on the summation.\n",
    "    \n",
    "    Returns Sn (float)\n",
    "    \"\"\"\n",
    "\n",
    "    # INSERT CODE HERE\n",
    "    Sn = 0\n",
    "    for n in xrange(1,N+1):\n",
    "        Sn += 1.0/(n*(n+1))\n",
    "    return Sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "T2-a",
     "locked": true,
     "points": 10,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "N = numpy.array([10**n for n in xrange(1,8)])\n",
    "answer = numpy.zeros((2, N.shape[0]))\n",
    "for (n, upper_bound) in enumerate(N):\n",
    "    answer[0, n] = sum_1(upper_bound)\n",
    "    answer[1, n] = sum_2(upper_bound)\n",
    "numpy.testing.assert_allclose(answer[0, :], numpy.array([0.9090909090909089, 0.9900990099009896, \n",
    "                                                         0.9990009990009996, 0.9999000099990004, \n",
    "                                                         0.9999900001000117, 0.9999990000010469,\n",
    "                                                         0.9999998999998143]))\n",
    "numpy.testing.assert_allclose(answer[1, :], numpy.array([0.9090909090909091, 0.9900990099009898, \n",
    "                                                         0.9990009990009997, 0.9999000099990007, \n",
    "                                                         0.9999900001000122, 0.9999990000010476, \n",
    "                                                         0.9999998999998153]))\n",
    "print \"Success!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Q2-b",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**(b)**  (5) Compute the absolute error between the two summation approaches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "A2-b",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def abs_error(N):\n",
    "    \"\"\"Compute the absolute error of the two sums defined as\n",
    "    \n",
    "    \\sum^N_{n=1} \\left [ \\frac{1}{n} - \\frac{1}{n+1} \\right ]\n",
    "    \n",
    "    and \n",
    "    \n",
    "    \\sum^N_{n=1} \\frac{1}{n (n + 1)}\n",
    "    \n",
    "    respectively for the given N.\n",
    "    \n",
    "    :Input:\n",
    "     *N* (int) The upper bound on the summation.\n",
    "    \n",
    "    Returns *error* (float)\n",
    "    \"\"\"\n",
    "    \n",
    "    # INSERT CODE HERE\n",
    "    f = sum_1(N)\n",
    "    f_hat = sum_2(N)\n",
    "    \n",
    "    \n",
    "    error = numpy.abs(f-f_hat)\n",
    "    #print f, ' - ', f_hat, ' = ', error\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "T2-b",
     "locked": true,
     "points": 5,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "N = numpy.array([10**n for n in xrange(1,8)])\n",
    "answer = numpy.zeros(N.shape)\n",
    "for (n, upper_bound) in enumerate(N):\n",
    "    answer[n] = abs_error(upper_bound)\n",
    "numpy.testing.assert_allclose(answer, numpy.array([1.1102230246251565e-16, 1.1102230246251565e-16, \n",
    "                                                   1.1102230246251565e-16, 3.3306690738754696e-16, \n",
    "                                                   4.4408920985006262e-16, 6.6613381477509392e-16, \n",
    "                                                   9.9920072216264089e-16]))\n",
    "print \"Success!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Q2-c",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "source": [
    "**(c)** (10) Plot the relative and absolute error versus $N$.  Also plot a line where $\\epsilon_{\\text{machine}}$ should be.  Comment on what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "A2-c",
     "locked": false,
     "points": 10,
     "solution": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtMAAAEdCAYAAADO/utoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREREREQF9H/X1E8sMKPXeAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0d63a324d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figwidth(fig.get_figwidth() * 2)\n",
    "\n",
    "# HINT!  Use the plotting function semilogx to plot the errors\n",
    "#        Also, do not forget to label your plot\n",
    "\n",
    "# INSERT CODE HERE\n",
    "axes = fig.add_subplot(1,1,1)\n",
    "abs_list = []\n",
    "rel_list = []\n",
    "n_list = []\n",
    "for x in xrange(1,8):\n",
    "    n = 10**x\n",
    "    n_list.append(n)\n",
    "    err = abs_error(n)\n",
    "    abs_list.append(err)\n",
    "    rel_list.append(err / numpy.abs(sum_1(n)))\n",
    "    abs_plot = axes.semilogx(n, err, 'bo')\n",
    "    rel_plot = axes.semilogx(n, err / numpy.abs(sum_1(n)), 'ro')\n",
    "    eps_plot = axes.semilogx(n, numpy.finfo(float).eps, 'go')\n",
    "axes.semilogx(n_list, abs_list)\n",
    "axes.semilogx(n_list, rel_list)\n",
    "axes.set_xlabel('N')\n",
    "axes.set_ylabel('error')\n",
    "axes.set_title('Plotting Error')\n",
    "axes.ticklabel_format(axis='y', style='sci', scilimits=(-1,1))\n",
    "axes.set_xlim(0, 10**8)\n",
    "plt.axhline(y = numpy.finfo(float).eps, xmin=0, xmax=10^8, linewidth=1, color = 'k')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**\n",
    "\n",
    "We can see that the relative error and absolute error are superimposed. At small n, we can notice the discrepancy between absolute and realtive error but then, due to the series converging at 1, they end up being nearly equal. This seems to tell us that the sum above, converges to 1, without actually solving for this convergence. In addition, we can see that for larger N, out error gets increasingly larger which makes sense as we had seen how this type of error compunds in class excercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Q2-d",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "source": [
    "**(d)** (5) Theorize what may have lead to the differences in answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "A2-d",
     "locked": false,
     "points": 5,
     "solution": true
    }
   },
   "source": [
    "The difference in answers occurs due to floating point error. We are approximating real numbers with finite precision causing to get these different values in the summation. Moreover, the more arithmetic we perform, the more this floating point error compunds leading to serious discrepancies. Therefore, because of this finite precision, we need to be cognizant of truncation and floating point error.\n",
    "\n",
    "For example, if we take the third term in the series, the first series has a fraction $\\frac{1}{3} - \\frac{1}{4}$ and the second sum has a fraction $\\frac{1}{12}$. As we saw in the class, $\\frac{1}{3}$ is a repeating decimal so due to our finite precision, we will have to truncate it causing us to get some error. Therefore, this trucncation and then calculation would account for the difference in answers between the two summations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Q3",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Question 3\n",
    "\n",
    "Following our discussion in lecture regarding approximating $e^x$ again consider the Taylor polynomial approximation:\n",
    "\n",
    "$$e^x \\approx T_n(x) = 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\cdots + \\frac{x^n}{n!}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Q3-a",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "source": [
    "**(a)** Derive the upper bound on the *relative error* assuming that $x > 0$ and\n",
    "\n",
    "$$R_n = \\frac{|e^x - T_n(x)|}{|e^x|}$$\n",
    "\n",
    "is given by\n",
    "\n",
    "$$R_n \\leq \\left | \\frac{x^{n+1}}{(n + 1)!} \\right |$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "A3-a",
     "locked": false,
     "points": 15,
     "solution": true
    }
   },
   "source": [
    "**(a)** Deriving the upper bound:\n",
    "\n",
    "$$ f(x) = T_N(x) + R_N(x)$$\n",
    "\n",
    "$$R_N(x) = \\frac{f^{(n+1)}(c) \\cdot (x - x_0)^{n+1}}{(n+1)!}$$\n",
    "\n",
    "$$R_N(x) = \\frac{e^cx^{n+1}}{(n+1)!}$$\n",
    "\n",
    "$$ R_N(x) = f(x) - T_N(x)$$\n",
    "\n",
    "$$R_N(x) = e^x - T_N(x)$$\n",
    "\n",
    "The Residual here appears exactly in our relative error equation:\n",
    "\n",
    "$$R_n = \\frac{|R_N(x)|}{|e^x|} =  \\left | \\frac{e^c}{e^x} \\right | \\left | \\frac{x^{n+1}}{(n+1)!} \\right |$$\n",
    "\n",
    "We must choose a c that will maximize the error. With a choice from [0,x], we must choose x since the denominator is exponentially decreasing driving any other choice to 0.\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$R_n \\leq \\left | \\frac{x^{n+1}}{(n + 1)!} \\right |$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Q3-b",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "source": [
    "**(b)** Show that for large $x$ and $n$, $r_n \\leq \\epsilon_{\\text{machine}}$ implies that we need at least $n > e \\cdot x$ terms in the series (where $e = \\text{exp}(1)$).\n",
    "\n",
    "*Hint* Use Stirling's approximation $log (n!) \\approx n~log~n - n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "A3-b",
     "locked": false,
     "points": 15,
     "solution": true
    }
   },
   "source": [
    "**(b)** We know the following:\n",
    "\n",
    "$$r_n \\leq \\epsilon_{\\text{machine}}$$ and $$r_n \\leq \\left | \\frac{x^{n+1}}{(n + 1)!} \\right |$$ \n",
    "\n",
    "Therefore, let\n",
    "\n",
    "$$\\left | \\frac{x^{n+1}}{(n + 1)!} \\right | \\leq \\epsilon_{\\text{machine}}$$\n",
    "\n",
    "$$log (x^{n+1}) - log (n+1!) \\leq log(\\epsilon_{\\text{machine}})$$\n",
    "\n",
    "By Stirling's Approximation:\n",
    "\n",
    "$$(n+1)log (x) - [(n+1) log (n+1) - (n+1)] \\leq log(\\epsilon_{\\text{machine}})$$\n",
    "\n",
    "$$(n+1) [log(x)-log(n+1)+1] \\leq log(\\epsilon_{\\text{machine}}) $$\n",
    "\n",
    "$$log(x) + 1 \\leq \\frac{log(\\epsilon_{\\text{machine}})}{n+1} + log(n+1)$$\n",
    "\n",
    "For large n, $\\frac{log(\\epsilon_{\\text{machine}})}{n+1}$ will go to 0 and $log(n+1) \\approx log(n)$:\n",
    "\n",
    "$$log(n) > log(x) + 1 \\rightarrow log(n) > log(e \\cdot x)$$\n",
    "\n",
    "$$n > e \\cdot x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Q3-c",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**(c)** Write a Python function that accurately computes $T_n$ to the specified relative error tolerance and returns both the estimate on the range and the number of terms in the series needed over the interval $[-2, 2]$.  Note that the testing tolerance will be $8 \\cdot \\epsilon_{\\text{machine}}$.\n",
    "\n",
    "Make sure to document your code including expected inputs, outputs, and assumptions being made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "A3-c",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# HINT: Think about how we evaluated polynomials efficiently in class\n",
    "\n",
    "'''Horners Method.\n",
    "\n",
    "Expected inputs: x as an ndarray and tolerance for error\n",
    "\n",
    "Outputs: \n",
    "    Taylor Approximations in the form of a list, showing all different calculations from varying x\n",
    "    N - number of steps before tolerance reached.\n",
    "'''\n",
    "\n",
    "import scipy.misc as misc\n",
    "\n",
    "def Tn_exp(x, tolerance=1e-3):\n",
    "    \"\"\" Returns the Evaluated Taylor Polynomial given \"\"\"\n",
    "    MAX_N = 100\n",
    "    N = MAX_N\n",
    "    \n",
    "    # INSERT CODE HERE\n",
    "    \"\"\"We are assuming x is an array here\"\"\"\n",
    "    y = numpy.ones(x.shape) * (1.0/misc.factorial(MAX_N))\n",
    "    \n",
    "    \"\"\"We go backwards in order to get our list of coefficients\"\"\"\n",
    "    for n in xrange(MAX_N - 1, -1, -1):\n",
    "        y = y * x + (1.0/misc.factorial(n))\n",
    "        if numpy.all(numpy.abs(numpy.exp(1) - y) / y ) < tolerance:\n",
    "            N = n\n",
    "            break\n",
    "    print N\n",
    "    Tn = y\n",
    "    return Tn, N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "T3-c",
     "locked": true,
     "points": 10,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "x = numpy.linspace(-2, 2, 100)\n",
    "tolerance = 8.0 * numpy.finfo(float).eps\n",
    "answer, N = Tn_exp(x, tolerance=tolerance)\n",
    "assert(numpy.all(numpy.abs(answer - numpy.exp(x)) / numpy.abs(numpy.exp(x)) < tolerance))\n",
    "print \"Success!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Q4",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "source": [
    "## Question 4\n",
    "\n",
    "Given the Taylor polynomial expansions\n",
    "\n",
    "$$\\frac{1}{1-\\Delta x} = 1 + \\Delta x + \\Delta x^2 + \\Delta x^3 + \\mathcal{O}(\\Delta x^4)$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\cosh \\Delta x = 1 - \\frac{\\Delta x^2}{2!} + \\frac{\\Delta x^4}{4!} + \\mathcal{O}(\\Delta x^6)$$\n",
    "\n",
    "determine the order of approximation for their sum and product (determine the exponent that belongs in the $\\mathcal{O}$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "A4",
     "locked": false,
     "points": 10,
     "solution": true
    }
   },
   "source": [
    "**Solution**\n",
    "\n",
    "We know the following:\n",
    "   \n",
    "$$f(\\Delta x) = p(\\Delta x) + O(\\Delta x^n)$$\n",
    "$$g(\\Delta x) = q(\\Delta x) + O(\\Delta x^m)$$\n",
    "$$r = \\min(n, m)$$ \n",
    "\n",
    "$$f+g = p + q + O(\\Delta x^r)$$\n",
    "\n",
    "$$f \\cdot g = p \\cdot q + p \\cdot O(\\Delta x^m) + q \\cdot O(\\Delta x^n) + O(\\Delta x^{n+m}) = p \\cdot q + O(\\Delta x^r)$$\n",
    "\n",
    "In this case:\n",
    "\n",
    "$$f(\\Delta x) = \\frac{1}{1-\\Delta x} ~~~~~~ p(\\Delta x) = 1 + \\Delta x + \\Delta x^2 + \\Delta x^3$$\n",
    "\n",
    "$$g(\\Delta x) = \\cosh \\Delta x ~~~~~~ q(\\Delta x) = 1 - \\frac{\\Delta x^2}{2!} + \\frac{\\Delta x^4}{4!}$$\n",
    "\n",
    "$$r = \\min(n, m) = \\min(4,6) = 4 \\leftarrow Sol.$$\n",
    "\n",
    "    f+g:\n",
    "    \n",
    "$$f + g = p + q + O(\\Delta x^r)$$\n",
    "$$= 1 + \\Delta x + \\Delta x^2 + \\Delta x^3 + 1 - \\frac{\\Delta x^2}{2!} + \\frac{\\Delta x^4}{4!} + O(\\Delta x^4)$$ \n",
    "$$= 2 + \\Delta x + \\frac{\\Delta x^2}{2} + \\Delta x^3 + O(\\Delta x^4)$$\n",
    "\n",
    "    fg:\n",
    "\n",
    "$$f \\cdot g = p \\cdot q + p \\cdot O(\\Delta x^m) + q \\cdot O(\\Delta x^n) + O(\\Delta x^{n+m}) = p \\cdot q + O(\\Delta x^r)$$\n",
    "$$(1 + \\Delta x + \\Delta x^2 + \\Delta x^3) \\cdot (1 - \\frac{\\Delta x^2}{2!} + \\frac{\\Delta x^4}{4!}) + (1 + \\Delta x + \\Delta x^2 + \\Delta x^3) \\cdot (\\mathcal{O}(\\Delta x^6)) + (1 - \\frac{\\Delta x^2}{2!} + \\frac{\\Delta x^4}{4!}) \\cdot (\\mathcal{O}(\\Delta x^4)) + \\mathcal{O}(\\Delta x^{10}) $$\n",
    "$$= (1 + \\Delta x + \\Delta x^2 + \\Delta x^3) \\cdot (1 - \\frac{\\Delta x^2}{2!} + \\frac{\\Delta x^4}{4!}) + (\\mathcal{O}(\\Delta x^r)$$\n",
    "$$r = \\min(n, m) = \\min(4,6) = 4 \\leftarrow Sol.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
